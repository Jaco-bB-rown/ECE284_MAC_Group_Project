{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tensorboardX import SummaryWriter      \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [20, 30]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a81c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 5.635 (5.635)\tData 5.333 (5.333)\tLoss 2.3662 (2.3662)\tPrec 13.281% (13.281%)\n",
      "Epoch: [0][100/391]\tTime 0.025 (0.082)\tData 0.000 (0.053)\tLoss 2.2247 (4.2881)\tPrec 9.375% (13.119%)\n",
      "Epoch: [0][200/391]\tTime 0.026 (0.054)\tData 0.000 (0.027)\tLoss 2.1856 (3.2433)\tPrec 14.844% (14.890%)\n",
      "Epoch: [0][300/391]\tTime 0.026 (0.044)\tData 0.000 (0.018)\tLoss 2.0861 (2.8800)\tPrec 18.750% (16.043%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.658 (4.658)\tLoss 2.0470 (2.0470)\tPrec 24.219% (24.219%)\n",
      " * Prec 19.840% \n",
      "best acc: 19.840000\n",
      "Epoch: [1][0/391]\tTime 5.517 (5.517)\tData 5.490 (5.490)\tLoss 2.1743 (2.1743)\tPrec 18.750% (18.750%)\n",
      "Epoch: [1][100/391]\tTime 0.025 (0.080)\tData 0.000 (0.055)\tLoss 2.1807 (2.1115)\tPrec 15.625% (18.348%)\n",
      "Epoch: [1][200/391]\tTime 0.025 (0.053)\tData 0.000 (0.028)\tLoss 2.0667 (2.1014)\tPrec 21.094% (19.263%)\n",
      "Epoch: [1][300/391]\tTime 0.027 (0.043)\tData 0.001 (0.019)\tLoss 2.0915 (2.0913)\tPrec 20.312% (19.895%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.525 (4.525)\tLoss 1.9816 (1.9816)\tPrec 21.875% (21.875%)\n",
      " * Prec 19.560% \n",
      "best acc: 19.840000\n",
      "Epoch: [2][0/391]\tTime 5.470 (5.470)\tData 5.386 (5.386)\tLoss 2.0013 (2.0013)\tPrec 17.188% (17.188%)\n",
      "Epoch: [2][100/391]\tTime 0.024 (0.080)\tData 0.000 (0.054)\tLoss 2.0489 (2.0376)\tPrec 23.438% (22.045%)\n",
      "Epoch: [2][200/391]\tTime 0.025 (0.053)\tData 0.000 (0.027)\tLoss 1.9993 (2.0313)\tPrec 21.875% (21.988%)\n",
      "Epoch: [2][300/391]\tTime 0.024 (0.043)\tData 0.000 (0.018)\tLoss 1.8755 (2.0207)\tPrec 22.656% (22.064%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.483 (4.483)\tLoss 1.9379 (1.9379)\tPrec 22.656% (22.656%)\n",
      " * Prec 24.280% \n",
      "best acc: 24.280000\n",
      "Epoch: [3][0/391]\tTime 5.414 (5.414)\tData 5.322 (5.322)\tLoss 2.0635 (2.0635)\tPrec 27.344% (27.344%)\n",
      "Epoch: [3][100/391]\tTime 0.024 (0.081)\tData 0.001 (0.053)\tLoss 2.1228 (1.9674)\tPrec 24.219% (24.567%)\n",
      "Epoch: [3][200/391]\tTime 0.025 (0.053)\tData 0.001 (0.027)\tLoss 2.1168 (1.9658)\tPrec 15.625% (24.160%)\n",
      "Epoch: [3][300/391]\tTime 0.024 (0.044)\tData 0.000 (0.018)\tLoss 1.9408 (1.9546)\tPrec 30.469% (24.541%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.480 (4.480)\tLoss 1.8777 (1.8777)\tPrec 27.344% (27.344%)\n",
      " * Prec 27.350% \n",
      "best acc: 27.350000\n",
      "Epoch: [4][0/391]\tTime 5.383 (5.383)\tData 5.292 (5.292)\tLoss 2.0460 (2.0460)\tPrec 23.438% (23.438%)\n",
      "Epoch: [4][100/391]\tTime 0.024 (0.080)\tData 0.000 (0.053)\tLoss 1.9469 (1.9135)\tPrec 28.906% (25.743%)\n",
      "Epoch: [4][200/391]\tTime 0.029 (0.053)\tData 0.000 (0.027)\tLoss 1.8807 (1.9038)\tPrec 20.312% (26.185%)\n",
      "Epoch: [4][300/391]\tTime 0.024 (0.043)\tData 0.000 (0.018)\tLoss 2.0093 (1.9017)\tPrec 24.219% (26.290%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.474 (4.474)\tLoss 1.8450 (1.8450)\tPrec 29.688% (29.688%)\n",
      " * Prec 28.160% \n",
      "best acc: 28.160000\n",
      "Epoch: [5][0/391]\tTime 5.393 (5.393)\tData 5.306 (5.306)\tLoss 1.9916 (1.9916)\tPrec 24.219% (24.219%)\n",
      "Epoch: [5][100/391]\tTime 0.024 (0.079)\tData 0.000 (0.053)\tLoss 1.8442 (1.8816)\tPrec 33.594% (26.516%)\n",
      "Epoch: [5][200/391]\tTime 0.025 (0.052)\tData 0.000 (0.027)\tLoss 1.8373 (1.8709)\tPrec 20.312% (26.924%)\n",
      "Epoch: [5][300/391]\tTime 0.025 (0.043)\tData 0.000 (0.018)\tLoss 1.8575 (1.8710)\tPrec 27.344% (27.253%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.484 (4.484)\tLoss 1.8739 (1.8739)\tPrec 20.312% (20.312%)\n",
      " * Prec 29.150% \n",
      "best acc: 29.150000\n",
      "Epoch: [6][0/391]\tTime 5.415 (5.415)\tData 5.328 (5.328)\tLoss 1.9724 (1.9724)\tPrec 22.656% (22.656%)\n",
      "Epoch: [6][100/391]\tTime 0.029 (0.080)\tData 0.000 (0.053)\tLoss 1.8349 (1.8289)\tPrec 27.344% (28.759%)\n",
      "Epoch: [6][200/391]\tTime 0.024 (0.053)\tData 0.000 (0.027)\tLoss 1.7841 (1.8223)\tPrec 32.031% (28.805%)\n",
      "Epoch: [6][300/391]\tTime 0.024 (0.044)\tData 0.000 (0.018)\tLoss 1.7706 (1.8164)\tPrec 33.594% (29.462%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.483 (4.483)\tLoss 1.7245 (1.7245)\tPrec 31.250% (31.250%)\n",
      " * Prec 30.380% \n",
      "best acc: 30.380000\n",
      "Epoch: [7][0/391]\tTime 5.402 (5.402)\tData 5.317 (5.317)\tLoss 1.7288 (1.7288)\tPrec 36.719% (36.719%)\n",
      "Epoch: [7][100/391]\tTime 0.024 (0.079)\tData 0.000 (0.053)\tLoss 1.7257 (1.7827)\tPrec 34.375% (31.196%)\n",
      "Epoch: [7][200/391]\tTime 0.024 (0.052)\tData 0.001 (0.027)\tLoss 1.9284 (1.7742)\tPrec 28.906% (31.355%)\n",
      "Epoch: [7][300/391]\tTime 0.025 (0.043)\tData 0.000 (0.018)\tLoss 1.8361 (1.7719)\tPrec 28.125% (31.408%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.547 (4.547)\tLoss 1.6847 (1.6847)\tPrec 34.375% (34.375%)\n",
      " * Prec 29.650% \n",
      "best acc: 30.380000\n",
      "Epoch: [8][0/391]\tTime 5.425 (5.425)\tData 5.337 (5.337)\tLoss 1.9065 (1.9065)\tPrec 29.688% (29.688%)\n",
      "Epoch: [8][100/391]\tTime 0.024 (0.080)\tData 0.001 (0.053)\tLoss 1.7339 (1.7498)\tPrec 35.156% (31.954%)\n",
      "Epoch: [8][200/391]\tTime 0.025 (0.052)\tData 0.001 (0.027)\tLoss 1.8296 (1.7445)\tPrec 24.219% (32.152%)\n",
      "Epoch: [8][300/391]\tTime 0.023 (0.043)\tData 0.000 (0.018)\tLoss 1.7571 (1.7389)\tPrec 27.344% (32.122%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.475 (4.475)\tLoss 1.6413 (1.6413)\tPrec 35.156% (35.156%)\n",
      " * Prec 34.060% \n",
      "best acc: 34.060000\n",
      "Epoch: [9][0/391]\tTime 5.491 (5.491)\tData 5.400 (5.400)\tLoss 1.6735 (1.6735)\tPrec 35.156% (35.156%)\n",
      "Epoch: [9][100/391]\tTime 0.032 (0.080)\tData 0.001 (0.054)\tLoss 1.6250 (1.7012)\tPrec 33.594% (33.857%)\n",
      "Epoch: [9][200/391]\tTime 0.030 (0.053)\tData 0.001 (0.027)\tLoss 1.7486 (1.7003)\tPrec 32.812% (34.107%)\n",
      "Epoch: [9][300/391]\tTime 0.025 (0.044)\tData 0.001 (0.018)\tLoss 1.7276 (1.6966)\tPrec 27.344% (34.370%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.604 (4.604)\tLoss 1.7537 (1.7537)\tPrec 28.906% (28.906%)\n",
      " * Prec 33.720% \n",
      "best acc: 34.060000\n",
      "Epoch: [10][0/391]\tTime 5.555 (5.555)\tData 5.455 (5.455)\tLoss 1.6427 (1.6427)\tPrec 38.281% (38.281%)\n",
      "Epoch: [10][100/391]\tTime 0.025 (0.081)\tData 0.000 (0.054)\tLoss 1.6553 (1.6740)\tPrec 36.719% (35.388%)\n",
      "Epoch: [10][200/391]\tTime 0.026 (0.053)\tData 0.001 (0.028)\tLoss 1.6243 (1.6557)\tPrec 40.625% (36.322%)\n",
      "Epoch: [10][300/391]\tTime 0.025 (0.044)\tData 0.001 (0.019)\tLoss 1.5738 (1.6444)\tPrec 33.594% (36.778%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.555 (4.555)\tLoss 1.5308 (1.5308)\tPrec 40.625% (40.625%)\n",
      " * Prec 38.940% \n",
      "best acc: 38.940000\n",
      "Epoch: [11][0/391]\tTime 5.410 (5.410)\tData 5.380 (5.380)\tLoss 1.6198 (1.6198)\tPrec 39.062% (39.062%)\n",
      "Epoch: [11][100/391]\tTime 0.027 (0.081)\tData 0.000 (0.054)\tLoss 1.6055 (1.5744)\tPrec 36.719% (39.735%)\n",
      "Epoch: [11][200/391]\tTime 0.026 (0.053)\tData 0.000 (0.027)\tLoss 1.4957 (1.5708)\tPrec 43.750% (39.898%)\n",
      "Epoch: [11][300/391]\tTime 0.024 (0.044)\tData 0.001 (0.018)\tLoss 1.6883 (1.5585)\tPrec 32.812% (40.609%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.531 (4.531)\tLoss 1.5873 (1.5873)\tPrec 39.844% (39.844%)\n",
      " * Prec 41.020% \n",
      "best acc: 41.020000\n",
      "Epoch: [12][0/391]\tTime 5.393 (5.393)\tData 5.351 (5.351)\tLoss 1.6305 (1.6305)\tPrec 36.719% (36.719%)\n",
      "Epoch: [12][100/391]\tTime 0.026 (0.081)\tData 0.000 (0.053)\tLoss 1.4230 (1.5164)\tPrec 45.312% (42.644%)\n",
      "Epoch: [12][200/391]\tTime 0.025 (0.053)\tData 0.000 (0.027)\tLoss 1.4417 (1.5031)\tPrec 55.469% (43.602%)\n",
      "Epoch: [12][300/391]\tTime 0.025 (0.044)\tData 0.001 (0.018)\tLoss 1.4095 (1.4871)\tPrec 53.125% (44.612%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.751 (4.751)\tLoss 1.5047 (1.5047)\tPrec 42.969% (42.969%)\n",
      " * Prec 44.860% \n",
      "best acc: 44.860000\n",
      "Epoch: [13][0/391]\tTime 5.683 (5.683)\tData 5.579 (5.579)\tLoss 1.4772 (1.4772)\tPrec 45.312% (45.312%)\n",
      "Epoch: [13][100/391]\tTime 0.025 (0.084)\tData 0.002 (0.056)\tLoss 1.4710 (1.4259)\tPrec 45.312% (46.643%)\n",
      "Epoch: [13][200/391]\tTime 0.025 (0.056)\tData 0.001 (0.028)\tLoss 1.3650 (1.4155)\tPrec 48.438% (47.283%)\n",
      "Epoch: [13][300/391]\tTime 0.026 (0.046)\tData 0.000 (0.019)\tLoss 1.3946 (1.4090)\tPrec 49.219% (47.703%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.756 (4.756)\tLoss 1.2518 (1.2518)\tPrec 53.125% (53.125%)\n",
      " * Prec 50.500% \n",
      "best acc: 50.500000\n",
      "Epoch: [14][0/391]\tTime 5.808 (5.808)\tData 5.638 (5.638)\tLoss 1.2200 (1.2200)\tPrec 56.250% (56.250%)\n",
      "Epoch: [14][100/391]\tTime 0.030 (0.084)\tData 0.001 (0.056)\tLoss 1.3225 (1.3541)\tPrec 46.875% (50.990%)\n",
      "Epoch: [14][200/391]\tTime 0.036 (0.056)\tData 0.001 (0.028)\tLoss 1.4505 (1.3323)\tPrec 47.656% (51.730%)\n",
      "Epoch: [14][300/391]\tTime 0.025 (0.046)\tData 0.000 (0.019)\tLoss 1.5393 (1.3297)\tPrec 46.875% (51.586%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.768 (4.768)\tLoss 1.3374 (1.3374)\tPrec 50.781% (50.781%)\n",
      " * Prec 52.650% \n",
      "best acc: 52.650000\n",
      "Epoch: [15][0/391]\tTime 5.737 (5.737)\tData 5.567 (5.567)\tLoss 1.3532 (1.3532)\tPrec 52.344% (52.344%)\n",
      "Epoch: [15][100/391]\tTime 0.031 (0.084)\tData 0.002 (0.055)\tLoss 1.4297 (1.2908)\tPrec 47.656% (53.434%)\n",
      "Epoch: [15][200/391]\tTime 0.027 (0.055)\tData 0.000 (0.028)\tLoss 1.3305 (1.2886)\tPrec 56.250% (53.312%)\n",
      "Epoch: [15][300/391]\tTime 0.028 (0.046)\tData 0.000 (0.019)\tLoss 1.1168 (1.2765)\tPrec 57.031% (53.662%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.736 (4.736)\tLoss 1.2583 (1.2583)\tPrec 57.812% (57.812%)\n",
      " * Prec 50.880% \n",
      "best acc: 52.650000\n",
      "Epoch: [16][0/391]\tTime 5.683 (5.683)\tData 5.633 (5.633)\tLoss 1.2038 (1.2038)\tPrec 57.031% (57.031%)\n",
      "Epoch: [16][100/391]\tTime 0.024 (0.085)\tData 0.001 (0.056)\tLoss 1.1861 (1.2196)\tPrec 60.938% (56.018%)\n",
      "Epoch: [16][200/391]\tTime 0.027 (0.056)\tData 0.000 (0.028)\tLoss 1.2604 (1.2289)\tPrec 57.031% (55.799%)\n",
      "Epoch: [16][300/391]\tTime 0.027 (0.046)\tData 0.000 (0.019)\tLoss 1.2451 (1.2198)\tPrec 57.812% (56.266%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.706 (4.706)\tLoss 1.2426 (1.2426)\tPrec 53.125% (53.125%)\n",
      " * Prec 53.750% \n",
      "best acc: 53.750000\n",
      "Epoch: [17][0/391]\tTime 5.805 (5.805)\tData 5.676 (5.676)\tLoss 1.2820 (1.2820)\tPrec 50.000% (50.000%)\n",
      "Epoch: [17][100/391]\tTime 0.027 (0.086)\tData 0.000 (0.057)\tLoss 1.1590 (1.1841)\tPrec 57.031% (56.954%)\n",
      "Epoch: [17][200/391]\tTime 0.034 (0.056)\tData 0.000 (0.029)\tLoss 1.3113 (1.1735)\tPrec 53.125% (57.529%)\n",
      "Epoch: [17][300/391]\tTime 0.031 (0.046)\tData 0.001 (0.019)\tLoss 1.2200 (1.1656)\tPrec 60.156% (58.010%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.829 (4.829)\tLoss 1.2375 (1.2375)\tPrec 57.031% (57.031%)\n",
      " * Prec 57.470% \n",
      "best acc: 57.470000\n",
      "Epoch: [18][0/391]\tTime 5.663 (5.663)\tData 5.558 (5.558)\tLoss 1.0485 (1.0485)\tPrec 62.500% (62.500%)\n",
      "Epoch: [18][100/391]\tTime 0.026 (0.083)\tData 0.001 (0.055)\tLoss 1.0830 (1.1209)\tPrec 61.719% (59.862%)\n",
      "Epoch: [18][200/391]\tTime 0.028 (0.055)\tData 0.000 (0.028)\tLoss 0.9611 (1.0991)\tPrec 67.969% (60.833%)\n",
      "Epoch: [18][300/391]\tTime 0.026 (0.045)\tData 0.000 (0.019)\tLoss 1.1592 (1.0903)\tPrec 61.719% (61.226%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.689 (4.689)\tLoss 1.1371 (1.1371)\tPrec 55.469% (55.469%)\n",
      " * Prec 61.610% \n",
      "best acc: 61.610000\n",
      "Epoch: [19][0/391]\tTime 5.668 (5.668)\tData 5.547 (5.547)\tLoss 1.0425 (1.0425)\tPrec 60.938% (60.938%)\n",
      "Epoch: [19][100/391]\tTime 0.026 (0.085)\tData 0.000 (0.055)\tLoss 1.0817 (1.0422)\tPrec 67.969% (63.181%)\n",
      "Epoch: [19][200/391]\tTime 0.025 (0.056)\tData 0.000 (0.028)\tLoss 0.9848 (1.0303)\tPrec 64.844% (63.608%)\n",
      "Epoch: [19][300/391]\tTime 0.028 (0.046)\tData 0.002 (0.019)\tLoss 0.9785 (1.0304)\tPrec 65.625% (63.434%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.678 (4.678)\tLoss 1.0071 (1.0071)\tPrec 65.625% (65.625%)\n",
      " * Prec 63.700% \n",
      "best acc: 63.700000\n",
      "Epoch: [20][0/391]\tTime 5.542 (5.542)\tData 5.464 (5.464)\tLoss 0.9765 (0.9765)\tPrec 65.625% (65.625%)\n",
      "Epoch: [20][100/391]\tTime 0.025 (0.082)\tData 0.001 (0.054)\tLoss 0.8706 (0.9321)\tPrec 71.094% (67.118%)\n",
      "Epoch: [20][200/391]\tTime 0.025 (0.054)\tData 0.000 (0.028)\tLoss 0.8228 (0.9103)\tPrec 71.094% (67.774%)\n",
      "Epoch: [20][300/391]\tTime 0.025 (0.045)\tData 0.000 (0.019)\tLoss 0.9364 (0.8998)\tPrec 64.844% (67.979%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.634 (4.634)\tLoss 0.7474 (0.7474)\tPrec 73.438% (73.438%)\n",
      " * Prec 69.470% \n",
      "best acc: 69.470000\n",
      "Epoch: [21][0/391]\tTime 5.507 (5.507)\tData 5.442 (5.442)\tLoss 0.7526 (0.7526)\tPrec 72.656% (72.656%)\n",
      "Epoch: [21][100/391]\tTime 0.024 (0.081)\tData 0.000 (0.054)\tLoss 0.9325 (0.8612)\tPrec 69.531% (69.609%)\n",
      "Epoch: [21][200/391]\tTime 0.027 (0.054)\tData 0.000 (0.027)\tLoss 0.8233 (0.8510)\tPrec 67.969% (69.963%)\n",
      "Epoch: [21][300/391]\tTime 0.032 (0.045)\tData 0.000 (0.018)\tLoss 0.8730 (0.8522)\tPrec 71.094% (70.009%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.558 (4.558)\tLoss 0.7421 (0.7421)\tPrec 72.656% (72.656%)\n",
      " * Prec 70.670% \n",
      "best acc: 70.670000\n",
      "Epoch: [22][0/391]\tTime 5.593 (5.593)\tData 5.497 (5.497)\tLoss 0.7966 (0.7966)\tPrec 72.656% (72.656%)\n",
      "Epoch: [22][100/391]\tTime 0.025 (0.082)\tData 0.000 (0.055)\tLoss 0.8621 (0.8312)\tPrec 71.875% (70.390%)\n",
      "Epoch: [22][200/391]\tTime 0.025 (0.054)\tData 0.000 (0.028)\tLoss 0.7186 (0.8322)\tPrec 75.000% (70.340%)\n",
      "Epoch: [22][300/391]\tTime 0.033 (0.045)\tData 0.001 (0.019)\tLoss 0.8417 (0.8257)\tPrec 67.188% (70.629%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.513 (4.513)\tLoss 0.7678 (0.7678)\tPrec 73.438% (73.438%)\n",
      " * Prec 71.580% \n",
      "best acc: 71.580000\n",
      "Epoch: [23][0/391]\tTime 5.628 (5.628)\tData 5.533 (5.533)\tLoss 0.7946 (0.7946)\tPrec 76.562% (76.562%)\n",
      "Epoch: [23][100/391]\tTime 0.026 (0.083)\tData 0.000 (0.055)\tLoss 0.8563 (0.8218)\tPrec 67.969% (71.016%)\n",
      "Epoch: [23][200/391]\tTime 0.024 (0.054)\tData 0.000 (0.028)\tLoss 0.8017 (0.8194)\tPrec 73.438% (71.047%)\n",
      "Epoch: [23][300/391]\tTime 0.024 (0.045)\tData 0.001 (0.019)\tLoss 0.8222 (0.8145)\tPrec 71.094% (71.211%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.520 (4.520)\tLoss 0.7477 (0.7477)\tPrec 70.312% (70.312%)\n",
      " * Prec 71.260% \n",
      "best acc: 71.580000\n",
      "Epoch: [24][0/391]\tTime 5.608 (5.608)\tData 5.518 (5.518)\tLoss 0.8362 (0.8362)\tPrec 71.094% (71.094%)\n",
      "Epoch: [24][100/391]\tTime 0.025 (0.083)\tData 0.001 (0.055)\tLoss 0.8014 (0.7918)\tPrec 75.000% (71.860%)\n",
      "Epoch: [24][200/391]\tTime 0.025 (0.054)\tData 0.001 (0.028)\tLoss 0.7731 (0.8042)\tPrec 71.094% (71.420%)\n",
      "Epoch: [24][300/391]\tTime 0.032 (0.044)\tData 0.001 (0.019)\tLoss 0.7660 (0.8040)\tPrec 71.094% (71.387%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.547 (4.547)\tLoss 0.7166 (0.7166)\tPrec 76.562% (76.562%)\n",
      " * Prec 71.750% \n",
      "best acc: 71.750000\n",
      "Epoch: [25][0/391]\tTime 5.472 (5.472)\tData 5.377 (5.377)\tLoss 0.7900 (0.7900)\tPrec 71.094% (71.094%)\n",
      "Epoch: [25][100/391]\tTime 0.025 (0.081)\tData 0.000 (0.054)\tLoss 0.8312 (0.7925)\tPrec 70.312% (71.952%)\n",
      "Epoch: [25][200/391]\tTime 0.024 (0.053)\tData 0.001 (0.027)\tLoss 0.8825 (0.7876)\tPrec 68.750% (72.116%)\n",
      "Epoch: [25][300/391]\tTime 0.024 (0.044)\tData 0.001 (0.018)\tLoss 0.8837 (0.7891)\tPrec 71.094% (72.046%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.534 (4.534)\tLoss 0.6980 (0.6980)\tPrec 76.562% (76.562%)\n",
      " * Prec 72.280% \n",
      "best acc: 72.280000\n",
      "Epoch: [26][0/391]\tTime 5.498 (5.498)\tData 5.403 (5.403)\tLoss 0.8366 (0.8366)\tPrec 69.531% (69.531%)\n",
      "Epoch: [26][100/391]\tTime 0.025 (0.081)\tData 0.001 (0.054)\tLoss 0.7266 (0.7862)\tPrec 75.000% (72.641%)\n",
      "Epoch: [26][200/391]\tTime 0.026 (0.053)\tData 0.000 (0.027)\tLoss 0.8402 (0.7816)\tPrec 71.875% (72.617%)\n",
      "Epoch: [26][300/391]\tTime 0.025 (0.044)\tData 0.000 (0.018)\tLoss 0.7856 (0.7796)\tPrec 74.219% (72.726%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.567 (4.567)\tLoss 0.6799 (0.6799)\tPrec 75.781% (75.781%)\n",
      " * Prec 72.710% \n",
      "best acc: 72.710000\n",
      "Epoch: [27][0/391]\tTime 5.489 (5.489)\tData 5.395 (5.395)\tLoss 0.7369 (0.7369)\tPrec 73.438% (73.438%)\n",
      "Epoch: [27][100/391]\tTime 0.025 (0.081)\tData 0.001 (0.054)\tLoss 0.7397 (0.7680)\tPrec 74.219% (72.718%)\n",
      "Epoch: [27][200/391]\tTime 0.025 (0.053)\tData 0.000 (0.027)\tLoss 0.9064 (0.7690)\tPrec 68.750% (72.862%)\n",
      "Epoch: [27][300/391]\tTime 0.024 (0.044)\tData 0.000 (0.018)\tLoss 0.8020 (0.7719)\tPrec 71.875% (72.817%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.504 (4.504)\tLoss 0.6489 (0.6489)\tPrec 78.906% (78.906%)\n",
      " * Prec 73.390% \n",
      "best acc: 73.390000\n",
      "Epoch: [28][0/391]\tTime 5.480 (5.480)\tData 5.387 (5.387)\tLoss 0.8676 (0.8676)\tPrec 72.656% (72.656%)\n",
      "Epoch: [28][100/391]\tTime 0.032 (0.081)\tData 0.000 (0.054)\tLoss 0.8139 (0.7592)\tPrec 68.750% (73.499%)\n",
      "Epoch: [28][200/391]\tTime 0.024 (0.053)\tData 0.000 (0.027)\tLoss 1.0301 (0.7520)\tPrec 64.844% (73.609%)\n",
      "Epoch: [28][300/391]\tTime 0.032 (0.044)\tData 0.000 (0.018)\tLoss 0.7259 (0.7544)\tPrec 73.438% (73.541%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.543 (4.543)\tLoss 0.7029 (0.7029)\tPrec 76.562% (76.562%)\n",
      " * Prec 74.030% \n",
      "best acc: 74.030000\n",
      "Epoch: [29][0/391]\tTime 5.547 (5.547)\tData 5.458 (5.458)\tLoss 0.8048 (0.8048)\tPrec 74.219% (74.219%)\n",
      "Epoch: [29][100/391]\tTime 0.031 (0.082)\tData 0.000 (0.054)\tLoss 0.6607 (0.7329)\tPrec 75.781% (74.188%)\n",
      "Epoch: [29][200/391]\tTime 0.023 (0.054)\tData 0.000 (0.028)\tLoss 0.5597 (0.7312)\tPrec 81.250% (74.223%)\n",
      "Epoch: [29][300/391]\tTime 0.024 (0.044)\tData 0.000 (0.019)\tLoss 0.6111 (0.7328)\tPrec 79.688% (74.154%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.563 (4.563)\tLoss 0.6648 (0.6648)\tPrec 76.562% (76.562%)\n",
      " * Prec 74.590% \n",
      "best acc: 74.590000\n",
      "Epoch: [30][0/391]\tTime 5.498 (5.498)\tData 5.408 (5.408)\tLoss 0.6825 (0.6825)\tPrec 75.781% (75.781%)\n",
      "Epoch: [30][100/391]\tTime 0.025 (0.081)\tData 0.000 (0.054)\tLoss 0.7359 (0.7203)\tPrec 67.969% (74.760%)\n",
      "Epoch: [30][200/391]\tTime 0.024 (0.053)\tData 0.000 (0.027)\tLoss 0.9602 (0.7206)\tPrec 65.625% (74.611%)\n",
      "Epoch: [30][300/391]\tTime 0.025 (0.044)\tData 0.000 (0.018)\tLoss 0.6690 (0.7160)\tPrec 76.562% (74.894%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.572 (4.572)\tLoss 0.6028 (0.6028)\tPrec 78.125% (78.125%)\n",
      " * Prec 75.390% \n",
      "best acc: 75.390000\n",
      "Epoch: [31][0/391]\tTime 5.545 (5.545)\tData 5.453 (5.453)\tLoss 0.6886 (0.6886)\tPrec 76.562% (76.562%)\n",
      "Epoch: [31][100/391]\tTime 0.025 (0.082)\tData 0.000 (0.054)\tLoss 0.7702 (0.7109)\tPrec 74.219% (74.930%)\n",
      "Epoch: [31][200/391]\tTime 0.024 (0.054)\tData 0.000 (0.028)\tLoss 0.8322 (0.7032)\tPrec 69.531% (75.319%)\n",
      "Epoch: [31][300/391]\tTime 0.024 (0.045)\tData 0.000 (0.019)\tLoss 0.6957 (0.7013)\tPrec 81.250% (75.462%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.536 (4.536)\tLoss 0.5996 (0.5996)\tPrec 77.344% (77.344%)\n",
      " * Prec 75.480% \n",
      "best acc: 75.480000\n",
      "Epoch: [32][0/391]\tTime 5.507 (5.507)\tData 5.401 (5.401)\tLoss 0.5666 (0.5666)\tPrec 81.250% (81.250%)\n",
      "Epoch: [32][100/391]\tTime 0.024 (0.081)\tData 0.000 (0.054)\tLoss 0.7387 (0.6715)\tPrec 77.344% (76.686%)\n",
      "Epoch: [32][200/391]\tTime 0.025 (0.053)\tData 0.000 (0.027)\tLoss 0.6445 (0.6908)\tPrec 75.000% (75.820%)\n",
      "Epoch: [32][300/391]\tTime 0.025 (0.044)\tData 0.000 (0.018)\tLoss 0.7273 (0.6953)\tPrec 75.000% (75.693%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.605 (4.605)\tLoss 0.6011 (0.6011)\tPrec 79.688% (79.688%)\n",
      " * Prec 75.260% \n",
      "best acc: 75.480000\n",
      "Epoch: [33][0/391]\tTime 5.459 (5.459)\tData 5.369 (5.369)\tLoss 0.6210 (0.6210)\tPrec 76.562% (76.562%)\n",
      "Epoch: [33][100/391]\tTime 0.026 (0.080)\tData 0.000 (0.053)\tLoss 0.7736 (0.6972)\tPrec 74.219% (75.387%)\n",
      "Epoch: [33][200/391]\tTime 0.025 (0.052)\tData 0.001 (0.027)\tLoss 0.7348 (0.6937)\tPrec 71.094% (75.645%)\n",
      "Epoch: [33][300/391]\tTime 0.032 (0.043)\tData 0.002 (0.018)\tLoss 0.7958 (0.6992)\tPrec 70.312% (75.470%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.518 (4.518)\tLoss 0.6145 (0.6145)\tPrec 76.562% (76.562%)\n",
      " * Prec 75.870% \n",
      "best acc: 75.870000\n",
      "Epoch: [34][0/391]\tTime 5.479 (5.479)\tData 5.380 (5.380)\tLoss 0.6309 (0.6309)\tPrec 75.781% (75.781%)\n",
      "Epoch: [34][100/391]\tTime 0.025 (0.081)\tData 0.001 (0.054)\tLoss 0.8085 (0.7045)\tPrec 70.312% (75.565%)\n",
      "Epoch: [34][200/391]\tTime 0.025 (0.053)\tData 0.000 (0.027)\tLoss 0.5665 (0.7007)\tPrec 82.812% (75.501%)\n",
      "Epoch: [34][300/391]\tTime 0.025 (0.044)\tData 0.001 (0.018)\tLoss 0.5486 (0.6984)\tPrec 80.469% (75.597%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.510 (4.510)\tLoss 0.5824 (0.5824)\tPrec 79.688% (79.688%)\n",
      " * Prec 75.880% \n",
      "best acc: 75.880000\n",
      "Epoch: [35][0/391]\tTime 5.522 (5.522)\tData 5.459 (5.459)\tLoss 0.6140 (0.6140)\tPrec 78.906% (78.906%)\n",
      "Epoch: [35][100/391]\tTime 0.025 (0.081)\tData 0.000 (0.054)\tLoss 0.5717 (0.6672)\tPrec 82.031% (76.764%)\n",
      "Epoch: [35][200/391]\tTime 0.025 (0.053)\tData 0.000 (0.027)\tLoss 0.8085 (0.6828)\tPrec 71.875% (75.991%)\n",
      "Epoch: [35][300/391]\tTime 0.024 (0.044)\tData 0.000 (0.018)\tLoss 0.6662 (0.6921)\tPrec 76.562% (75.651%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.489 (4.489)\tLoss 0.6004 (0.6004)\tPrec 80.469% (80.469%)\n",
      " * Prec 75.700% \n",
      "best acc: 75.880000\n",
      "Epoch: [36][0/391]\tTime 5.395 (5.395)\tData 5.345 (5.345)\tLoss 0.7211 (0.7211)\tPrec 72.656% (72.656%)\n",
      "Epoch: [36][100/391]\tTime 0.024 (0.080)\tData 0.000 (0.053)\tLoss 0.6164 (0.6941)\tPrec 76.562% (75.820%)\n",
      "Epoch: [36][200/391]\tTime 0.034 (0.053)\tData 0.000 (0.027)\tLoss 0.6490 (0.6839)\tPrec 76.562% (76.116%)\n",
      "Epoch: [36][300/391]\tTime 0.024 (0.044)\tData 0.000 (0.018)\tLoss 0.7008 (0.6903)\tPrec 74.219% (75.911%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.589 (4.589)\tLoss 0.5705 (0.5705)\tPrec 80.469% (80.469%)\n",
      " * Prec 75.570% \n",
      "best acc: 75.880000\n",
      "Epoch: [37][0/391]\tTime 5.369 (5.369)\tData 5.323 (5.323)\tLoss 0.8016 (0.8016)\tPrec 69.531% (69.531%)\n",
      "Epoch: [37][100/391]\tTime 0.025 (0.079)\tData 0.000 (0.053)\tLoss 0.5759 (0.6851)\tPrec 81.250% (75.967%)\n",
      "Epoch: [37][200/391]\tTime 0.024 (0.052)\tData 0.000 (0.027)\tLoss 0.6656 (0.6867)\tPrec 78.906% (76.011%)\n",
      "Epoch: [37][300/391]\tTime 0.027 (0.044)\tData 0.001 (0.018)\tLoss 0.6571 (0.6891)\tPrec 75.000% (75.864%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.493 (4.493)\tLoss 0.6078 (0.6078)\tPrec 78.906% (78.906%)\n",
      " * Prec 75.620% \n",
      "best acc: 75.880000\n",
      "Epoch: [38][0/391]\tTime 5.413 (5.413)\tData 5.373 (5.373)\tLoss 0.7002 (0.7002)\tPrec 80.469% (80.469%)\n",
      "Epoch: [38][100/391]\tTime 0.025 (0.079)\tData 0.000 (0.053)\tLoss 0.6257 (0.6997)\tPrec 83.594% (75.364%)\n",
      "Epoch: [38][200/391]\tTime 0.024 (0.052)\tData 0.000 (0.027)\tLoss 0.6991 (0.6869)\tPrec 75.781% (75.785%)\n",
      "Epoch: [38][300/391]\tTime 0.025 (0.043)\tData 0.000 (0.018)\tLoss 0.5894 (0.6875)\tPrec 78.906% (75.882%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.476 (4.476)\tLoss 0.6355 (0.6355)\tPrec 80.469% (80.469%)\n",
      " * Prec 75.640% \n",
      "best acc: 75.880000\n",
      "Epoch: [39][0/391]\tTime 5.399 (5.399)\tData 5.308 (5.308)\tLoss 0.5212 (0.5212)\tPrec 80.469% (80.469%)\n",
      "Epoch: [39][100/391]\tTime 0.023 (0.080)\tData 0.000 (0.053)\tLoss 0.6792 (0.6956)\tPrec 77.344% (75.758%)\n",
      "Epoch: [39][200/391]\tTime 0.024 (0.052)\tData 0.000 (0.027)\tLoss 0.8230 (0.6898)\tPrec 70.312% (75.851%)\n",
      "Epoch: [39][300/391]\tTime 0.024 (0.043)\tData 0.000 (0.018)\tLoss 0.6202 (0.6877)\tPrec 79.688% (75.906%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 4.461 (4.461)\tLoss 0.5885 (0.5885)\tPrec 81.250% (81.250%)\n",
      " * Prec 76.150% \n",
      "best acc: 76.150000\n"
     ]
    }
   ],
   "source": [
    "lr = 8e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 40\n",
    "best_prec = 0\n",
    "\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "# weight decay: for regularization to prevent overfitting\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')   \n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    \n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_10864\\564645170.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9097/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        #print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_q = model.features[3].weight_q\n",
    "w_alpha = model.features[3].weight_quant.wgt_alpha\n",
    "w_bit = 4\n",
    "\n",
    "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))\n",
    "#print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = save_output.outputs[1][0]\n",
    "act_alpha  = model.features[3].act_alpha\n",
    "act_bit = 4\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "\n",
    "act_int = act_q / (act_alpha / (2**act_bit-1))\n",
    "#print(act_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "victorian-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is provided\n",
    "\n",
    "conv_int = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, padding=1)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "conv_int.bias = model.features[3].bias\n",
    "output_int = conv_int(act_int)\n",
    "output_recovered = output_int * (act_alpha / (2**act_bit-1)) * (w_alpha / (2**(w_bit-1)-1))\n",
    "#print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8340, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## This cell is provided\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, padding=1)\n",
    "conv_ref.weight = model.features[3].weight_q\n",
    "conv_ref.bias = model.features[3].bias\n",
    "output_ref = conv_ref(act)\n",
    "#print(output_ref)\n",
    "\n",
    "print(abs((output_ref - output_recovered)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subsequent-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
    "# a_int.size() = [64, 32, 32]\n",
    "\n",
    "# conv_int.weight.size() = torch.Size([64, 64, 3, 3])  <- output_ch, input_ch, ki, kj\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([64, 64, 9])\n",
    "                      \n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel \n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(len(icg)/array_size))\n",
    "oc_tileg = range(int(len(ocg)/array_size))\n",
    "\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]\n",
    "\n",
    "\n",
    "a_tile = torch.zeros(len(ic_tileg), array_size,    a_pad.size(1)).cuda() \n",
    "w_tile = torch.zeros(len(oc_tileg)*len(ic_tileg), array_size, array_size, len(kijg)).cuda() \n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    a_tile[ic_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        w_tile[oc_tile*len(oc_tileg) + ic_tile,:,:,:] = w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, :]\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for kij in kijg:\n",
    "    for ic_tile in ic_tileg:       # Tiling into array_sizeXarray_size array\n",
    "        for oc_tile in oc_tileg:   # Tiling into array_sizeXarray_size array        \n",
    "            for nij in p_nijg:       # time domain, sequentially given input\n",
    "                    m = nn.Linear(array_size, array_size, bias=False)\n",
    "                    #m.weight = torch.nn.Parameter(w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, kij])\n",
    "                    m.weight = torch.nn.Parameter(w_tile[len(oc_tileg)*oc_tile+ic_tile,:,:,kij])\n",
    "                    psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile,:,nij]).cuda()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32\n",
    "\n",
    "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1)\n",
    "o_nijg = range(o_ni_dim**2)    \n",
    "    \n",
    "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
    "  \n",
    "   \n",
    "### SFP accumulation ###\n",
    "for o_nij in o_nijg: \n",
    "    for kij in kijg:\n",
    "        for ic_tile in ic_tileg:    \n",
    "            for oc_tile in oc_tileg:   \n",
    "                out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] = out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] + \\\n",
    "                psum[ic_tile, oc_tile, :, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
    "                ## 4th index = (int(o_nij/30)*32 + o_nij%30) + (int(kij/3)*32 + kij%3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0091, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_2D = torch.reshape(out, (out.size(0), o_ni_dim, -1))\n",
    "difference = (out_2D - output_int[0,:,:,:])\n",
    "print(difference.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endangered-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "tile_id = 0 \n",
    "nij = 200 # just a random number\n",
    "X = a_tile[tile_id,:,nij:nij+64]  # [tile_num, array row num, time_steps]\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('activation.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(round(X[7-j,i].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accomplished-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "tile_id = 0 \n",
    "kij = 0\n",
    "W = w_tile[tile_id,:,:,kij]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('weight.txt', 'w') #write to file\n",
    "file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(W.size(0)):  # col #\n",
    "    for j in range(W.size(1)): # row #\n",
    "        temp=round(W[i,j].item())\n",
    "        if(temp < 0 ):\n",
    "            temp=temp+16\n",
    "        W_bin = '{0:04b}'.format(temp)\n",
    "        for k in range(bit_precision):\n",
    "            file.write(W_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coastal-panel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., -0., 0., 1., 1., 0., -0., 0.], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0,:] # check this number with your 2nd line in weight.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stupid-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0 \n",
    "oc_tile_id = 0 \n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 200\n",
    "psum_tile = psum[ic_tile_id,oc_tile_id,:,nij:nij+64,kij]  \n",
    "# psum[len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)]\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('psum.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(psum_tile.size(1)):  # nijg #\n",
    "    for j in range(psum_tile.size(0)): # row #\n",
    "        temp=round(psum_tile[j,i].item())\n",
    "        if(temp < 0 ):\n",
    "            temp=temp+65536\n",
    "        W_bin = '{0:016b}'.format(temp)\n",
    "        for k in range(bit_precision):\n",
    "            file.write(W_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
